---
title: "Voting Prediction"
date: "`r format(Sys.Date())`"
author: "Austin Pham"
output:
  html_document:
    df_print: tibble
    number_sections: yes
    theme: simplex
    toc: yes
    toc_depth: 2
params:
    kaggle: TRUE
---

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(caretEnsemble)
library(xgboost)
library(ranger)
library(rpart)
library(dlookr)
library(MLmetrics)
tidymodels_prefer()
set.seed(42)
```

Let's read in our data

```{r}
train <- read_csv("heart/heart_train.csv")
test <- read_csv("heart/heart_test.csv")
test$num <- test$id
```
############
EDA

```{r eval = FALSE}
train %>%
  eda_web_report(target = "num", output_dir = "./", output_file = "EDA.html", theme = "blue")
train %>%
  eda_paged_report(target = "num", output_dir = "./", output_file = "EDA.pdf", theme = "blue")
```

Lets remove some outliers and missing data from our training.

```{r}
outliers <- c()
train <- train[!train$id %in% outliers, ]
```

Set our training/testing sets.

```{r}
nTrain <- nrow(train)
nTest <- nrow(test)
heart_df <- rbind(train, test)
```

Our recipe will:

1. Remove all predictors with only one observation (zv)
2. Perform variable transformations (YeoJohnson & Normalize)
3. Create few higher-order terms for non-linear relationships

```{r}
heart_df2 <- recipe(num ~ ., data = heart_df) %>%
  update_role(id, new_role = "id_variable") %>%
  step_zv(all_predictors()) %>% 
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  prep() %>%
  juice()

heart_training <- head(heart_df2, nTrain)
heart_testing_raw <- tail(heart_df2, nTest)
```

Correlation Plots

```{r eval=FALSE}
cor <- correlate(heart_training) %>%
  arrange(coef_corr) %>%
  slice(which(row_number() %% 2 == 1))
view(cor)
```

Variable Importance with Random Forests

```{r eval=FALSE}
rf_res1 <- ranger(num ~ . - id, , data = heart_training, importance = "impurity_corrected")
importance(rf_res1) %>% 
  enframe("Variable", "Importance") %>%
  arrange(desc(Importance)) %>%
  slice(1:50) %>% ggplot(aes(x = Variable, y = Importance, fill = Importance)) + geom_col() + coord_flip() + 
  labs(title = "Random Forest Variable Importance")
```

Finalize the recipe

```{r}
tmp_rec1 <- recipe(num ~ ., data = heart_training) %>%
    step_rm(id) %>%
    step_zv(all_predictors()) %>% 
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors())

tmp_rec2 <- recipe(num ~ ., data = heart_training) %>%
    update_role(id, new_role = "id_variable") %>%
    step_zv(all_predictors()) %>% 
    step_YeoJohnson(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors())

tmp_rec1 %>%
    check_missing(all_predictors()) %>%
    prep()
```

Recipe

Inputs:

      role #variables
   outcome          1
 predictor         19

Training data contained 227 data points and no missing data.

Operations:

Variables removed id [trained]
Zero variance filter removed <none> [trained]
Yeo-Johnson transformation on age, trestbps, chol, thalach, oldpeak [trained]
Centering and scaling for age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea... [trained]
Dummy variables from <none> [trained]
Check missing values for age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea... [trained]


```{r}
heart_rec1 <- prep(tmp_rec1)
heart_rec2 <- prep(tmp_rec2)
heart_juiced <- juice(heart_rec1)
heart_juiced$num <- as.factor(heart_juiced$num)
levels(heart_juiced$num) <- c("good", "bad")
heart_testing <- bake(heart_rec2, heart_testing_raw)
```

Let's create an ensemble model. Predictions are made from several tuned models on the entire training data set. We create a new data set with three variables (one prediction from each of the models). These variables are used as predictors for the output and the new ensemble model is trained on this data set.

To predict on testing data, we 1) predict testing data using the individual models then 2) save the predictions and combine them to make the final predictions using the trained ensemble model. 


```{r}
set.seed(42)
trControl <- trainControl(
    method = "repeatedcv",
    savePredictions = "final",
    index = createMultiFolds(heart_juiced$num, k = 10, times = 3),
    allowParallel = TRUE,
    verboseIter = TRUE,
    summaryFunction = prSummary,
    classProbs = TRUE
)
```

####################################
MODEL 1: knn (k-Nearest Neighbors ) 
####################################

227 samples
 18 predictor
  2 classes: 'good', 'bad' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 203, 205, 205, 205, 204, 205, ... 
Resampling results across tuning parameters:

    k       AUC Precision    Recall         F
14 14 0.6799454 0.8241068 0.8833333 0.8487065
11 11 0.5947830 0.8351968 0.8724359 0.8486986
13 13 0.6691279 0.8279962 0.8752137 0.8469970
12 12 0.6356578 0.8317723 0.8722222 0.8466209
7   7 0.4909407 0.8292461 0.8662393 0.8437626
10 10 0.5818076 0.8238112 0.8694444 0.8412263
5   5 0.4351146 0.8287730 0.8615385 0.8401439
8   8 0.5262362 0.8263205 0.8636752 0.8399762
9   9 0.5735760 0.8229216 0.8664530 0.8398543
16 16 0.7082583 0.8169418 0.8696581 0.8376258
20 20 0.7373510 0.8095456 0.8777778 0.8369976
4   4 0.3887784 0.8296720 0.8536325 0.8367458
15 15 0.6971120 0.8146341 0.8645299 0.8333997
18 18 0.7220310 0.8076400 0.8696581 0.8324705
19 19 0.7198843 0.8035748 0.8722222 0.8315520

F was used to select the optimal model using the largest value.
The final value used for the model was k = 14.

```{r eval=FALSE}
knnGrid <- expand.grid(
    k = seq(1, 20, length.out = 20))
knnModel <- caretList(
    num ~ ., data = heart_juiced,
    trControl = trControl,
    metric = "F",
    tuneList = list(
        logitBoost = caretModelSpec(method = "knn", tuneGrid = knnGrid)
    )
)
logitBoostModel
head(knnModel$logitBoost$results[order(knnModel$logitBoost$results$F, decreasing = TRUE),1:5], 10)
```

####################################
MODEL 2: svmRadial (Support Vector Machines with Radial Basis Function Kernel)
####################################

227 samples
 18 predictor
  2 classes: 'good', 'bad' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 203, 205, 205, 205, 204, 205, ... 
Resampling results across tuning parameters:

          sigma   C       AUC Precision    Recall         F
10 3.010101e-05 200 0.8376480 0.8462949 0.8773504 0.8566919
12 5.020202e-05 125 0.8379840 0.8462949 0.8773504 0.8566919
13 5.020202e-05 150 0.8385122 0.8462949 0.8773504 0.8566919
37 1.507071e-04 125 0.8341701 0.8423215 0.8801282 0.8561961
21 9.040404e-05 100 0.8351136 0.8419745 0.8801282 0.8560910
9  3.010101e-05 175 0.8368334 0.8440221 0.8773504 0.8556050
11 5.020202e-05 100 0.8370924 0.8440221 0.8773504 0.8556050
41 1.708081e-04 100 0.8360802 0.8438082 0.8773504 0.8554039
14 5.020202e-05 175 0.8358219 0.8432646 0.8775641 0.8553964
36 1.507071e-04 100 0.8365871 0.8431896 0.8773504 0.8553416

F was used to select the optimal model using the largest value.
The final values used for the model were sigma = 5.020202e-05 and C = 125.

```{r eval=FALSE}
svmGrid <- expand.grid(
    sigma = seq(0.00001, 0.002, length.out = 100),
    C = seq(100, 200, length.out = 5))
svmModel <- caretList(
    num ~ ., data = heart_juiced,
    trControl = trControl,
    metric = "F",
    tuneList = list(
        svm = caretModelSpec(method = "svmRadial", tuneGrid = svmGrid)
    )
)
head(svmModel$svm$results[order(svmModel$svm$results$F, decreasing = TRUE),1:6], 10)
```

####################################
MODEL 3: glmnet (Lasso and Elastic-Net Regularized Generalized Linear Models)
####################################

227 samples
 18 predictor
  2 classes: 'good', 'bad' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 203, 205, 205, 205, 204, 205, ... 
Resampling results across tuning parameters:

        alpha      lambda       AUC Precision    Recall         F
60  0.5526316 0.010000000 0.8403399 0.8325864 0.8608974 0.8405455
80  0.5789474 0.010000000 0.8403399 0.8325864 0.8608974 0.8405455
99  0.6052632 0.009473737 0.8399909 0.8325864 0.8608974 0.8405455
100 0.6052632 0.010000000 0.8400372 0.8325864 0.8608974 0.8405455
119 0.6315789 0.009473737 0.8395279 0.8325864 0.8608974 0.8405455
120 0.6315789 0.010000000 0.8399464 0.8325864 0.8608974 0.8405455
139 0.6578947 0.009473737 0.8400372 0.8325864 0.8608974 0.8405455
140 0.6578947 0.010000000 0.8399464 0.8325864 0.8608974 0.8405455
158 0.6842105 0.008947474 0.8398160 0.8325864 0.8608974 0.8405455
159 0.6842105 0.009473737 0.8397714 0.8325864 0.8608974 0.8405455

F was used to select the optimal model using the largest value.
The final values used for the model were alpha = 0.5526316 and lambda = 0.01.

```{r eval=FALSE}
glmnetGrid <- expand.grid(
    alpha = seq(0.5, 1, length.out = 20),
    lambda = seq(1e-06, 0.01, length.out = 20))
glmnet_model <- caretList(
    num ~ ., data = heart_juiced,
    trControl = trControl,
    metric = "F",
    tuneList = list(
        glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid)
    )
)
head(glmnet_model$glmnet$results[order(glmnet_model$glmnet$results$F, decreasing = TRUE),1:6], 10)
```

####################################
MODEL 4: ranger (Random Forest)
####################################

227 samples
 18 predictor
  2 classes: 'good', 'bad' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 203, 205, 205, 205, 204, 205, ... 
Resampling results across tuning parameters:

   mtry       AUC Precision    Recall         F
1     1 0.8529410 0.8197632 0.8801282 0.8429047
2     2 0.8458670 0.8137493 0.8608974 0.8302577
3     4 0.8374491 0.8104607 0.8557692 0.8262167
4     6 0.8346690 0.8075256 0.8559829 0.8244563
6    10 0.8246158 0.8052126 0.8425214 0.8166352
5     8 0.8267564 0.8018130 0.8448718 0.8166331
10   50 0.8121732 0.8041861 0.8425214 0.8166277
13   80 0.8202499 0.8005748 0.8455128 0.8165214
12   70 0.8144949 0.7976951 0.8482906 0.8157958
11   60 0.8169881 0.7936733 0.8480769 0.8141270

F was used to select the optimal model using the largest value.
The final value used for the model was mtry = 1.

```{r eval=FALSE}
rfGrid <- expand.grid(
    mtry = c(1, 2, 4, 6, 8, seq(10, 100, 10)))
rf_model <- caretList(
    num ~ ., data = heart_juiced,
    trControl = trControl,
    metric = "F",
    tuneList = list(
        rf = caretModelSpec(method = "rf", tuneGrid = rfGrid)
    )
)
head(rf_model$rf$results[order(rf_model$rf$results$F, decreasing = TRUE),1:5], 10)
```

####################################
MODEL 5: Elasticnet
####################################

Elasticnet 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

       lambda  fraction       RMSE  Rsquared
20 0.05555556 1.0000000 0.07989453 0.7406886
19 0.05555556 0.8888889 0.08033308 0.7378090
18 0.05555556 0.7777778 0.08089545 0.7341090
17 0.05555556 0.6666667 0.08195793 0.7271759
16 0.05555556 0.5555556 0.08386029 0.7147223
30 0.11111111 1.0000000 0.08417252 0.7132750
29 0.11111111 0.8888889 0.08455164 0.7103964
28 0.11111111 0.7777778 0.08516977 0.7057766
27 0.11111111 0.6666667 0.08630048 0.6975507
15 0.05555556 0.4444444 0.08699106 0.6935419
40 0.16666667 1.0000000 0.08713244 0.6961632
39 0.16666667 0.8888889 0.08747299 0.6931017
38 0.16666667 0.7777778 0.08810797 0.6877746
26 0.11111111 0.5555556 0.08840908 0.6823302
37 0.16666667 0.6666667 0.08932217 0.6780933

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 1 and lambda = 0.05555556.

```{r eval=FALSE}
xgGrid <- expand.grid(
    nrounds = c(10, 100, 10),
    max_depth = seq(2, 8, 1),
    eta = c(0.1, 0.2, 0.3),
    gamma = 10^c(-1:3),
    colsample_bytree = seq(0, 1, 0.2),
    min_child_weight = 1,
    subsample = 1)
xg_model <- caretList(
    num ~ ., data = heart_juiced,
    trControl = trControl,
    metric = "F",
    tuneList = list(
        enet = caretModelSpec(method = "xgbTree", tuneGrid = xgGrid)
    )
)

```

####################################
MODEL 6: Bayesian Ridge Regression (Model Averaged) 
####################################

2331 samples
 110 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2099, 2099, 2096, 2099, 2097, 2097, ... 
Resampling results:

  RMSE        Rsquared   MAE       
  0.07427017  0.7866994  0.05729397

```{r eval=FALSE}
bayeRidge_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        bayeRidge = caretModelSpec(method = "blassoAveraged")
    )
)
bayeRidge_model$bayeRidge
```

####################################
MODEL 7: Ensembles of Generalized Linear Models (method = 'randomGLM')
####################################

For classification and regression using package randomGLM with tuning parameters:

Interaction Order (maxInteractionOrder, numeric)

```{r eval=FALSE}
randomGLM_Grid <- expand.grid(maxInteractionOrder = 2)
randomGLM_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        randomGLM = caretModelSpec(method = "randomGLM", tuneGrid = randomGLM_Grid)
    )
)
head(randomGLM_model$randomGLM$results[order(randomGLM_model$randomGLM$results$RMSE),1:4], 15)
```

####################################
MODEL 8: Gaussian Process with Radial Basis Function Kernel (method = 'gaussprRadial')
####################################

Gaussian Process with Radial Basis Function Kernel 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

    sigma       RMSE  Rsquared        MAE
3 0.00335 0.08278861 0.7269400 0.06223496
4 0.00500 0.08354950 0.7224938 0.06239918
2 0.00170 0.08394214 0.7196370 0.06361436
1 0.00005 0.11198840 0.5090974 0.08523536

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sigma = 0.00335.

```{r eval=FALSE}
gauss_Grid <- expand.grid(sigma = seq(from = 5e-5, to = 5e-3, length.out = 4))
gauss_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
    )
)
head(gauss_model$gauss$results[order(gauss_model$gauss$results$RMSE),1:4], 15)
```

####################################
MODEL 9: Radial Basis Function Kernel Regularized Least Squares (method = 'krlsRadial')
####################################

For regression using packages KRLS and kernlab with tuning parameters:

Regularization Parameter (lambda, numeric)

Sigma (sigma, numeric)

```{r eval=FALSE}
krls_Grid <- expand.grid(sigma = c(10, 20, 30),
                        lambda = c(1e-5, 1e-3, 1e-1))
krls_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        krls = caretModelSpec(method = "krlsRadial", tuneGrid = krls_Grid)
    )
)
head(krls_model$krls$results[order(krls_model$krls$results$RMSE),1:4], 15)
```

####################################
MODEL 10: Ensemble Model
####################################

Now, let's form our final model

eXtreme Gradient Boosting 

   max_depth colsample_bytree min_child_weight subsample nrounds       RMSE
7          6            0.625                4       0.5    2000 0.06282319
1          4            0.500                4       0.5    2000 0.06282756
3          4            0.750                4       0.5    2000 0.06297950

Support Vector Machines with Radial Basis Function Kernel 

          sigma   C       RMSE
1  0.0008000000 100 0.06819234
2  0.0008000000 114 0.06848837
4  0.0009333333 100 0.06881245

glmnet 

   alpha       lambda       RMSE  Rsquared
16  1.00 1.232847e-04 0.07144448 0.7923810
15  1.00 1.105149e-04 0.07148570 0.7921851
12  0.95 1.232847e-04 0.07149542 0.7921349

rf

  mtry splitrule min.node.size       RMSE
7   64  variance             2 0.07019780
9   64  variance             6 0.07033081
8   64  variance             4 0.07037421

Elastic Net

       lambda  fraction       RMSE  Rsquared
20 0.05555556 1.0000000 0.07989453 0.7406886
19 0.05555556 0.8888889 0.08033308 0.7378090
18 0.05555556 0.7777778 0.08089545 0.7341090

Gaussian Process

    sigma       RMSE  Rsquared        MAE
3 0.00335 0.08278861 0.7269400 0.06223496
4 0.00500 0.08354950 0.7224938 0.06239918





```{r}
# model 1 (xgboost)
xgbTreeGrid <- expand.grid(nrounds = 2000, max_depth = 4, eta = 0.02,
    gamma = 0, colsample_bytree = 0.625, subsample = 0.5, min_child_weight = 4)
# model 2 (svmRadial)
svmGrid <- expand.grid(sigma = 0.0008, C = 100)
# model 3 (glmnet)
glmnetGrid <- expand.grid(alpha = 1.0, lambda = 1.232847e-04)
# model 4 (ranger)
rfGrid <- expand.grid(mtry = 64, splitrule = "variance", min.node.size = 2)
# model 5 (elastic)
enetGrid <- expand.grid(fraction = 1, lambda = 0.05555556)
# model 6 (bayseianRidge): NA
# model 7 (randomGLM): NA
# model 8 (gaussprRadial):
gauss_Grid <- expand.grid(sigma = 0.00335)
# model 9 (krlsRadial): NA
stacked_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        xgb = caretModelSpec(method = "xgbTree", tuneGrid = xgbTreeGrid),
        svm = caretModelSpec(method = "svmRadial", tuneGrid = svmGrid),
        glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid),
        rf = caretModelSpec(method = "ranger", tuneGrid = rfGrid),
        enet = caretModelSpec(method = "enet", tuneGrid = enetGrid),
        # bayeRidge = caretModelSpec(method = "blassoAveraged"),
        gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
        # krls = caretModelSpec(method = "krlsRadial", tuneGrid = krls_Grid)
    )
)
# saveRDS(stacked_model, "stacked_model.rds")
stacked_model <- readRDS("submissions/stacked_model.rds")
```

Finally, the predictions are ensembled using `caretEnsemble()`.

```{r caret-stack}
vote_stack <- caretEnsemble(stacked_model)
vote_pred <- predict(vote_stack, newdata = vote_testing) %>%
    bind_cols(vote_testing) %>%
    select(Id = Id, Predicted = ...1)
write.csv(vote_pred, file = "cyclip.csv", row.names = FALSE)
```

We can also have a look at the models

```{r}
vote_stack$ens_model
```

Generalized Linear Model 

4624 samples
   6 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 4624, 4624, 4624, 4624, 4624, 4624, ... 
Resampling results:

  RMSE        Rsquared   MAE       
  0.06045091  0.8501321  0.04554867



```{r eval=FALSE}
bwplot(resamples(vote_stack$models), metric = "RMSE")
modelCor(resamples(vote_stack$models))
```

              xgb         svm     glmnet         rf       enet       gauss
xgb     1.0000000  0.64143035  0.5989465  0.8248589  0.5939343 -0.13702886
svm     0.6414303  1.00000000  0.5667066  0.4468704  0.6110026 -0.06522863
glmnet  0.5989465  0.56670660  1.0000000  0.3509400  0.8732723 -0.26642132
rf      0.8248589  0.44687038  0.3509400  1.0000000  0.3893840 -0.26473684
enet    0.5939343  0.61100264  0.8732723  0.3893840  1.0000000 -0.21038590
gauss  -0.1370289 -0.06522863 -0.2664213 -0.2647368 -0.2103859  1.00000000
