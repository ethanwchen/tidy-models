---
title: "Voting Prediction"
date: "`r format(Sys.Date())`"
author: "Austin Pham"
output:
  html_document:
    df_print: tibble
    number_sections: yes
    theme: simplex
    toc: yes
    toc_depth: 2
params:
    kaggle: TRUE
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.align = "center")
library(conflicted)
library(tidymodels)
library(workflows)
library(tune)
library(readr)
library(forcats)
library(doFuture)
library(caret)
library(caretEnsemble)
library(patchwork)
library(tabnet)
library(xgboost)
library(dlookr)
library(tidyverse)
library(tidymodels)
library(vip)
library(skimr)
library(skimr)
library(timetk)
library(ranger)
library(kernlab)
library(tictoc)
library(rpart)
library(CORElearn)
library(caretEnsemble)
tidymodels_prefer()
theme_set(theme_light())
set.seed(42)
```

Let's read in our data

```{r}
train <- read_csv("ensemble/train.csv")
test <- read_csv("ensemble/test.csv")
test$percent_dem <- test$id
train <- rename(train, Id = "id")
test <- rename(test, Id = "id")
train <- rename(train, target = "percent_dem")
test <- rename(test, target = "percent_dem")
```

Lets remove some outliers and missing data from our training.

```{r}
outliers <- c(57, 439, 461, 693, 863, 936, 1236, 1254, 1305, 1380, 1709, 1727, 1847, 2027, 2104, 2132, 2643, 2980)
train <- train[!train$Id %in% outliers, ]
train <- train[-c(1322), ] # remove NA
```

Set our training/testing sets.

```{r}
nTrain <- nrow(train)
nTest <- nrow(test)
vote_df <- rbind(train, test)
```

Our recipe will:

1. Remove all predictors with only one observation (zv)
2. Perform variable transformations (YeoJohnson & Normalize)
3. Create few higher-order terms for non-linear relationships

```{r}
vote_df2 <- recipe(target ~ ., data = vote_df) %>%
  update_role(Id, new_role = "id_variable") %>%
  step_zv(all_predictors()) %>% 
  step_YeoJohnson(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  prep() %>%
  juice()

vote_training <- head(vote_df2, nTrain)
vote_testing_raw <- tail(vote_df2, nTest) %>% select(-target)
```

Correlation Plots

```{r eval=FALSE}
cor <- correlate(vote_training) %>%
  arrange(coef_corr) %>%
  slice(which(row_number() %% 2 == 1))
view(cor)
```

Variable Importance with Random Forests

```{r eval=FALSE}
rf_plot <- vote_training
names(rf_plot) <- paste("rf", names(rf_plot), sep = "_")
rf_res1 <- ranger(rf_target ~ . - rf_Id, , data = rf_plot, importance = "impurity_corrected")
importance(rf_res1) %>% 
  enframe("Variable", "Importance") %>%
  arrange(desc(Importance)) %>%
  slice(1:50) %>% ggplot(aes(x = Variable, y = Importance, fill = Importance)) + geom_col() + coord_flip() + 
  labs(title = "Random Forest Variable Importance")
```

Finalize the recipe

```{r}
tmp_rec1 <- recipe(target ~ ., data = vote_training) %>%
    step_rm(Id) %>%
    step_zv(all_predictors()) %>%
    step_YeoJohnson(all_predictors()) %>%
    step_normalize(all_predictors())

tmp_rec2 <- recipe(target ~ ., data = vote_training) %>%
    update_role(Id, new_role = "id_variable") %>%
    step_zv(all_predictors()) %>%
    step_YeoJohnson(all_predictors()) %>%
    step_normalize(all_predictors())

tmp_rec1 %>%
    check_missing(all_predictors()) %>%
    prep()
```

Recipe

Inputs:

      role #variables
   outcome          1
 predictor        111

Training data contained 2331 data points and no missing data.

Operations:

Variables removed Id [trained]
Zero variance filter removed <none> [trained]
Yeo-Johnson transformation on male, female, age0, age5, age10, age15, age20, age25, age3... [trained]
Centering and scaling for male, female, age0, age5, age10, age15, age20, age25, age35, age45,... [trained]
Check missing values for male, female, age0, age5, age10, age15, age20, age25, age35, age45,... [trained]


```{r}
vote_rec1 <- prep(tmp_rec1)
vote_rec2 <- prep(tmp_rec2)
vote_juiced <- juice(vote_rec1)
vote_testing <- bake(vote_rec2, vote_testing_raw)
```

Let's create an ensemble model. Predictions are made from several tuned models on the entire training data set. We create a new data set with three variables (one prediction from each of the models). These variables are used as predictors for the output and the new ensemble model is trained on this data set.

To predict on testing data, we 1) predict testing data using the individual models then 2) save the predictions and combine them to make the final predictions using the trained ensemble model. 


```{r}
set.seed(4312)
trControl <- trainControl(
    method = "cv",
    savePredictions = "final",
    index = createMultiFolds(vote_juiced$target, k = 10, times = 2),
    allowParallel = TRUE,
    verboseIter = TRUE
)
```

####################################
MODEL 1: xgbTree (eXtreme Gradient Boosting) 
####################################

eXtreme Gradient Boosting 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

   max_depth colsample_bytree min_child_weight subsample nrounds       RMSE
7          6            0.625                4       0.5    2000 0.06282319
1          4            0.500                4       0.5    2000 0.06282756
3          4            0.750                4       0.5    2000 0.06297950
4          4            0.875                4       0.5    2000 0.06312865
5          4            1.000                4       0.5    2000 0.06317782
8          6            0.750                4       0.5    2000 0.06319506
9          6            0.875                4       0.5    2000 0.06323865
10         6            1.000                4       0.5    2000 0.06328220
2          4            0.625                4       0.5    2000 0.06334981
6          6            0.500                4       0.5    2000 0.06338109
12         8            0.625                4       0.5    2000 0.06363014
14         8            0.875                4       0.5    2000 0.06384987
17        10            0.625                4       0.5    2000 0.06388951
13         8            0.750                4       0.5    2000 0.06389628
11         8            0.500                4       0.5    2000 0.06392901

Tuning parameter 'nrounds' was held constant at a value of 2000
Tuning parameter 'eta' was

Tuning parameter 'min_child_weight' was held constant at a value of 4
Tuning parameter
 'subsample' was held constant at a value of 0.5
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 2000, max_depth = 6, eta = 0.02, gamma =
 0, colsample_bytree = 0.625, min_child_weight = 4 and subsample = 0.5.

```{r eval=FALSE}
xgbTreeGrid <- expand.grid(
    nrounds = 2000,
    max_depth = c(4, 6, 8, 10),
    eta = 0.02,
    gamma = 0,
    colsample_bytree = seq(0.5, 1, length.out = 5),
    subsample = 0.5,
    min_child_weight = 4)
xgbTree_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        xgb = caretModelSpec(method = "xgbTree", tuneGrid = xgbTreeGrid)
    )
)
head(xgbTree_model$xgb$results[order(xgbTree_model$xgb$results$RMSE),c(2,4:8)], 15)
```

####################################
MODEL 2: svmRadial (Support Vector Machines with Radial Basis Function Kernel)
####################################

Support Vector Machines with Radial Basis Function Kernel 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

          sigma   C       RMSE
1  0.0008000000 100 0.06819234
2  0.0008000000 114 0.06848837
4  0.0009333333 100 0.06881245
3  0.0008000000 128 0.06884455
5  0.0009333333 114 0.06931182
7  0.0010666667 100 0.06964079
6  0.0009333333 128 0.06982645
8  0.0010666667 114 0.07024468
10 0.0012000000 100 0.07046148
9  0.0010666667 128 0.07082260
11 0.0012000000 114 0.07106632
12 0.0012000000 128 0.07158540

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 8e-04 and C = 100.

```{r eval=FALSE}
svmGrid <- expand.grid(
    sigma = seq(0.0008, 0.0012, length.out = 4),
    C = seq(100, 128, length.out = 3))
svm_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        svm = caretModelSpec(method = "svmRadial", tuneGrid = svmGrid)
    )
)
head(svm_model$svm$results[order(svm_model$svm$results$RMSE),1:3], 15)
```

####################################
MODEL 3: glmnet (Lasso and Elastic-Net Regularized Generalized Linear Models)
####################################

glmnet 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

   alpha       lambda       RMSE  Rsquared
16  1.00 1.232847e-04 0.07144448 0.7923810
15  1.00 1.105149e-04 0.07148570 0.7921851
12  0.95 1.232847e-04 0.07149542 0.7921349
4   0.85 1.232847e-04 0.07150308 0.7920678
8   0.90 1.232847e-04 0.07150382 0.7920700
11  0.95 1.105149e-04 0.07153985 0.7919266
14  1.00 9.774513e-05 0.07154648 0.7918844
3   0.85 1.105149e-04 0.07154817 0.7918748
7   0.90 1.105149e-04 0.07155305 0.7918245
10  0.95 9.774513e-05 0.07159763 0.7916420
2   0.85 9.774513e-05 0.07160230 0.7916379
6   0.90 9.774513e-05 0.07161733 0.7915157
13  1.00 8.497534e-05 0.07165439 0.7913771
5   0.90 8.497534e-05 0.07168438 0.7912282
9   0.95 8.497534e-05 0.07168529 0.7912366

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 1 and lambda = 0.0001232847.

```{r eval=FALSE}
glmnetGrid <- expand.grid(
    alpha = seq(0.85, 1, length.out = 4),
    lambda = seq(8.497534e-05, 0.0001232847, length.out = 4))
glmnet_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid)
    )
)
head(glmnet_model$glmnet$results[order(glmnet_model$glmnet$results$RMSE),1:4], 15)
```

####################################
MODEL 4: ranger (Random Forest)
####################################

Random Forest 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

  mtry splitrule min.node.size       RMSE
7   64  variance             2 0.07019780
9   64  variance             6 0.07033081
8   64  variance             4 0.07037421
5   52  variance             4 0.07048135
6   52  variance             6 0.07053440
4   52  variance             2 0.07074304
1   40  variance             2 0.07098718
2   40  variance             4 0.07100097
3   40  variance             6 0.07119625

Tuning parameter 'splitrule' was held constant at a value of variance
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 64, splitrule = variance and min.node.size = 2.

```{r eval=FALSE}
rfGrid <- expand.grid(
    mtry = c(40, 52, 64),
    splitrule = "variance",
    min.node.size = c(2, 4, 6))
rf_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        rf = caretModelSpec(method = "ranger", tuneGrid = rfGrid)
    )
)
head(rf_model$rf$results[order(rf_model$rf$results$RMSE),1:4], 15)

```

####################################
MODEL 5: Elasticnet
####################################

Elasticnet 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

       lambda  fraction       RMSE  Rsquared
20 0.05555556 1.0000000 0.07989453 0.7406886
19 0.05555556 0.8888889 0.08033308 0.7378090
18 0.05555556 0.7777778 0.08089545 0.7341090
17 0.05555556 0.6666667 0.08195793 0.7271759
16 0.05555556 0.5555556 0.08386029 0.7147223
30 0.11111111 1.0000000 0.08417252 0.7132750
29 0.11111111 0.8888889 0.08455164 0.7103964
28 0.11111111 0.7777778 0.08516977 0.7057766
27 0.11111111 0.6666667 0.08630048 0.6975507
15 0.05555556 0.4444444 0.08699106 0.6935419
40 0.16666667 1.0000000 0.08713244 0.6961632
39 0.16666667 0.8888889 0.08747299 0.6931017
38 0.16666667 0.7777778 0.08810797 0.6877746
26 0.11111111 0.5555556 0.08840908 0.6823302
37 0.16666667 0.6666667 0.08932217 0.6780933

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 1 and lambda = 0.05555556.

```{r eval=FALSE}
enetGrid <- expand.grid(
    fraction = seq(from = 0, to = 2, length.out = 10),
    lambda = seq(from = 0, to = 0.5, length.out = 10))
enet_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        enet = caretModelSpec(method = "enet", tuneGrid = enetGrid)
    )
)
head(enet_model$enet$results[order(enet_model$enet$results$RMSE),1:4], 15)
```

####################################
MODEL 6: Bayesian Ridge Regression (Model Averaged) 
####################################

2331 samples
 110 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2099, 2099, 2096, 2099, 2097, 2097, ... 
Resampling results:

  RMSE        Rsquared   MAE       
  0.07427017  0.7866994  0.05729397

```{r eval=FALSE}
bayeRidge_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        bayeRidge = caretModelSpec(method = "blassoAveraged")
    )
)
bayeRidge_model$bayeRidge
```

####################################
MODEL 7: Ensembles of Generalized Linear Models (method = 'randomGLM')
####################################

For classification and regression using package randomGLM with tuning parameters:

Interaction Order (maxInteractionOrder, numeric)

```{r eval=FALSE}
randomGLM_Grid <- expand.grid(maxInteractionOrder = 2)
randomGLM_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        randomGLM = caretModelSpec(method = "randomGLM", tuneGrid = randomGLM_Grid)
    )
)
head(randomGLM_model$randomGLM$results[order(randomGLM_model$randomGLM$results$RMSE),1:4], 15)
```

####################################
MODEL 8: Gaussian Process with Radial Basis Function Kernel (method = 'gaussprRadial')
####################################

Gaussian Process with Radial Basis Function Kernel 

2312 samples
 213 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 2080, 2080, 2082, 2081, 2081, 2081, ... 
Resampling results across tuning parameters:

    sigma       RMSE  Rsquared        MAE
3 0.00335 0.08278861 0.7269400 0.06223496
4 0.00500 0.08354950 0.7224938 0.06239918
2 0.00170 0.08394214 0.7196370 0.06361436
1 0.00005 0.11198840 0.5090974 0.08523536

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sigma = 0.00335.

```{r eval=FALSE}
gauss_Grid <- expand.grid(sigma = seq(from = 5e-5, to = 5e-3, length.out = 4))
gauss_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
    )
)
head(gauss_model$gauss$results[order(gauss_model$gauss$results$RMSE),1:4], 15)
```

####################################
MODEL 9: Radial Basis Function Kernel Regularized Least Squares (method = 'krlsRadial')
####################################

For regression using packages KRLS and kernlab with tuning parameters:

Regularization Parameter (lambda, numeric)

Sigma (sigma, numeric)

```{r eval=FALSE}
krls_Grid <- expand.grid(sigma = c(10, 20, 30),
                        lambda = c(1e-5, 1e-3, 1e-1))
krls_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        krls = caretModelSpec(method = "krlsRadial", tuneGrid = krls_Grid)
    )
)
head(krls_model$krls$results[order(krls_model$krls$results$RMSE),1:4], 15)
```

####################################
MODEL 10: Ensemble Model
####################################

Now, let's form our final model

eXtreme Gradient Boosting 

   max_depth colsample_bytree min_child_weight subsample nrounds       RMSE
7          6            0.625                4       0.5    2000 0.06282319
1          4            0.500                4       0.5    2000 0.06282756
3          4            0.750                4       0.5    2000 0.06297950

Support Vector Machines with Radial Basis Function Kernel 

          sigma   C       RMSE
1  0.0008000000 100 0.06819234
2  0.0008000000 114 0.06848837
4  0.0009333333 100 0.06881245

glmnet 

   alpha       lambda       RMSE  Rsquared
16  1.00 1.232847e-04 0.07144448 0.7923810
15  1.00 1.105149e-04 0.07148570 0.7921851
12  0.95 1.232847e-04 0.07149542 0.7921349

rf

  mtry splitrule min.node.size       RMSE
7   64  variance             2 0.07019780
9   64  variance             6 0.07033081
8   64  variance             4 0.07037421

Elastic Net

       lambda  fraction       RMSE  Rsquared
20 0.05555556 1.0000000 0.07989453 0.7406886
19 0.05555556 0.8888889 0.08033308 0.7378090
18 0.05555556 0.7777778 0.08089545 0.7341090

Gaussian Process

    sigma       RMSE  Rsquared        MAE
3 0.00335 0.08278861 0.7269400 0.06223496
4 0.00500 0.08354950 0.7224938 0.06239918





```{r}
# model 1 (xgboost)
xgbTreeGrid <- expand.grid(nrounds = 2000, max_depth = 4, eta = 0.02,
    gamma = 0, colsample_bytree = 0.625, subsample = 0.5, min_child_weight = 4)
# model 2 (svmRadial)
svmGrid <- expand.grid(sigma = 0.0008, C = 100)
# model 3 (glmnet)
glmnetGrid <- expand.grid(alpha = 1.0, lambda = 1.232847e-04)
# model 4 (ranger)
rfGrid <- expand.grid(mtry = 64, splitrule = "variance", min.node.size = 2)
# model 5 (elastic)
enetGrid <- expand.grid(fraction = 1, lambda = 0.05555556)
# model 6 (bayseianRidge): NA
# model 7 (randomGLM): NA
# model 8 (gaussprRadial):
gauss_Grid <- expand.grid(sigma = 0.00335)
# model 9 (krlsRadial): NA
stacked_model <- caretList(
    target ~ ., data = vote_juiced,
    trControl = trControl,
    metric = "RMSE",
    tuneList = list(
        xgb = caretModelSpec(method = "xgbTree", tuneGrid = xgbTreeGrid),
        svm = caretModelSpec(method = "svmRadial", tuneGrid = svmGrid),
        glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid),
        rf = caretModelSpec(method = "ranger", tuneGrid = rfGrid),
        enet = caretModelSpec(method = "enet", tuneGrid = enetGrid),
        # bayeRidge = caretModelSpec(method = "blassoAveraged"),
        gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
        # krls = caretModelSpec(method = "krlsRadial", tuneGrid = krls_Grid)
    )
)
# saveRDS(stacked_model, "stacked_model.rds")
stacked_model <- readRDS("submissions/stacked_model.rds")
```

Finally, the predictions are ensembled using `caretEnsemble()`.

```{r caret-stack}
vote_stack <- caretEnsemble(stacked_model)
vote_pred <- predict(vote_stack, newdata = vote_testing) %>%
    bind_cols(vote_testing) %>%
    select(Id = Id, Predicted = ...1)
write.csv(vote_pred, file = "cyclip.csv", row.names = FALSE)
```

We can also have a look at the models

```{r}
vote_stack$ens_model
```

Generalized Linear Model 

4624 samples
   6 predictor

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 4624, 4624, 4624, 4624, 4624, 4624, ... 
Resampling results:

  RMSE        Rsquared   MAE       
  0.06045091  0.8501321  0.04554867



```{r eval=FALSE}
bwplot(resamples(vote_stack$models), metric = "RMSE")
modelCor(resamples(vote_stack$models))
```

              xgb         svm     glmnet         rf       enet       gauss
xgb     1.0000000  0.64143035  0.5989465  0.8248589  0.5939343 -0.13702886
svm     0.6414303  1.00000000  0.5667066  0.4468704  0.6110026 -0.06522863
glmnet  0.5989465  0.56670660  1.0000000  0.3509400  0.8732723 -0.26642132
rf      0.8248589  0.44687038  0.3509400  1.0000000  0.3893840 -0.26473684
enet    0.5939343  0.61100264  0.8732723  0.3893840  1.0000000 -0.21038590
gauss  -0.1370289 -0.06522863 -0.2664213 -0.2647368 -0.2103859  1.00000000
